{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import KNNBaseline\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('ml-latest-small/ratings.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "moviesDF = pd.read_csv(\"ml-latest-small/movies.csv\", sep=\",\")\n",
    "moviesDF.movieId = moviesDF.movieId.astype(int)\n",
    "moviesDF.set_index(\"movieId\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplification of the evaluation case: predict last-k for each user\n",
    "dfTrain = pd.DataFrame()\n",
    "dfTest = pd.DataFrame()\n",
    "for currUser in df.userId.unique():\n",
    "    dataCurrUser = df[df.userId == currUser]\n",
    "    currUserTrain = dataCurrUser.iloc[:-10]\n",
    "    currUserTest = dataCurrUser.iloc[-10:]\n",
    "    dfTrain = dfTrain.append(currUserTrain)\n",
    "    dfTest = dfTest.append(currUserTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '3', '6', ..., '160836', '163937', '163981'], dtype='<U21')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allObjects = df.movieId.unique().astype(str)\n",
    "allTestSetUsers = dfTest.userId.unique().astype(str)\n",
    "allObjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain.to_csv('ml-latest-small/ratingsTrain.csv', index=False)\n",
    "dfTest.to_csv('ml-latest-small/ratingsTest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'ml-latest-small/ratingsTrain.csv'\n",
    "reader = Reader(line_format='user item rating timestamp', sep=',', skip_lines=1)\n",
    "\n",
    "data = Dataset.load_from_file(file_path, reader=reader)\n",
    "trainset = data.build_full_trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def novelty(perUserRecommendations):\n",
    "    # novelty = -log(num_users who have rated the item / num_users)\n",
    "    num_users = len(dfTrain.userId.unique())\n",
    "    acc_novelty = 0\n",
    "    for pred in perUserRecommendations:\n",
    "        num_rated = len(df[df[\"movieId\"] == int(pred.iid)])\n",
    "        acc_novelty -= np.log2(num_rated/num_users)\n",
    "    # return average novelty\n",
    "    return acc_novelty / len(perUserRecommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(perUserRecommendations):\n",
    "    hits = 0\n",
    "    for pred in perUserRecommendations:\n",
    "        if ((dfTest['userId'] == int(pred.uid)) & (dfTest['movieId'] == int(pred.iid))).any():\n",
    "            hits += 1\n",
    "    return hits / len(perUserRecommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_relevance_scores(recommendations: list) -> list:\n",
    "    result = []\n",
    "    for recommendation in recommendations:\n",
    "        ratingDf = dfTest[dfTest.userId == int(recommendation.uid)]\n",
    "        ratingDf = ratingDf[ratingDf.movieId == int(recommendation.iid)]\n",
    "\n",
    "        # recommendation iif is relevant if has been reviewed by the user uid\n",
    "        relevance = 1.0 if not ratingDf.empty else 0.0\n",
    "        result.append(relevance)\n",
    "\n",
    "    return np.array(result)\n",
    "\n",
    "def ndcg(perUserRecommendations):\n",
    "\n",
    "    def dcg(rel_scores):\n",
    "        pos = np.arange(1, rel_scores.shape[0] + 1)\n",
    "        return np.sum(\n",
    "          rel_scores / np.log2(pos + 1)\n",
    "        )\n",
    "\n",
    "    relevance_scores = binary_relevance_scores(perUserRecommendations)\n",
    "    hits = int(np.sum(relevance_scores))\n",
    "    # all relevant movies are placed first\n",
    "    ideal_relevance_scores = np.array(\n",
    "        ([1] * hits) + ([0] * (len(perUserRecommendations) - hits))\n",
    "    )\n",
    "    actual_dcg, ideal_dcg = dcg(relevance_scores), dcg(ideal_relevance_scores)\n",
    "\n",
    "    if actual_dcg == 0.0:\n",
    "        return 0.0\n",
    "\n",
    "    return actual_dcg / ideal_dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-dbb922332651>:15: RuntimeWarning: invalid value encountered in true_divide\n",
      "  rating_mat = rating_mat / (np.sum(rating_mat ** 2, axis=1) ** 0.5).reshape((-1,1))\n"
     ]
    }
   ],
   "source": [
    "# Precompute dense rating matrix\n",
    "movieIds = dfTrain['movieId'].unique()\n",
    "userIds = dfTrain['userId'].unique()\n",
    "\n",
    "def movieIdToRowIndex(i):\n",
    "    return np.where(i == movieIds)[0][0]\n",
    "def userIdToRowIndex(i):\n",
    "    return np.where(i == userIds)[0][0]\n",
    "\n",
    "movieIdsMapped = np.array(list(map(movieIdToRowIndex, dfTrain['movieId'].tolist())), dtype=np.int64).flatten()\n",
    "userIdsMapped = np.array(list(map(userIdToRowIndex, dfTrain['userId'].tolist())), dtype=np.int64).flatten()\n",
    "rating_mat = np.zeros((movieIds.shape[0], userIds.shape[0]), dtype=np.int8)\n",
    "\n",
    "rating_mat[movieIdsMapped, userIdsMapped] = dfTrain[\"rating\"].tolist()\n",
    "rating_mat = rating_mat / (np.sum(rating_mat ** 2, axis=1) ** 0.5).reshape((-1,1))\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return (1 + np.dot(a,b)) / 2 # expecting normalized vectors\n",
    "     \n",
    "class DiverseRecommend:\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def __call__(self, perUserPredictions, top_k):\n",
    "        #def diverse_recommend(perUserPredictions, top_k, alpha=0.5):\n",
    "        max_est = max(map(lambda x: x.est, perUserPredictions))\n",
    "        alpha = self.alpha\n",
    "        res = []\n",
    "        res_ids = []\n",
    "        res_row_indeces = []\n",
    "        perUserPredictionsSorted = sorted(perUserPredictions, key=lambda x: x.est, reverse=True)[:top_k * 10]\n",
    "        for i in range(top_k):\n",
    "            curr_max = -1\n",
    "            curr = None\n",
    "            \n",
    "            if len(res_row_indeces) > 0:\n",
    "                simMatrix = np.max(cosine_similarity(rating_mat, rating_mat[res_row_indeces].T), axis=1)\n",
    "                cosSimMean = np.mean(simMatrix)\n",
    "                \n",
    "            for pred in perUserPredictionsSorted:\n",
    "                predIid = int(pred.iid)\n",
    "                # optimization step\n",
    "                if curr_max > alpha * (pred.est / max_est):\n",
    "                    break\n",
    "\n",
    "                if predIid not in res_ids:\n",
    "                    max_cos = 0\n",
    "                    # Compute diversity only for the extended result and we have ranking vector\n",
    "                    # for the prediction\n",
    "                    if len(res_row_indeces) > 0 and np.any(np.where(predIid == movieIds)[0]):\n",
    "                        #max_cos = np.max(cosine_similarity(rating_mat[movieIdToRowIndex(predIid)], rating_mat[res_row_indeces].T))\n",
    "                        max_cos = simMatrix[movieIdToRowIndex(predIid)]\n",
    "                        v = alpha * (pred.est / max_est) - (1 - alpha) * max_cos\n",
    "                    elif len(res_row_indeces) > 0:\n",
    "                        v = alpha * (pred.est / max_est) - (1 - alpha) * cosSimMean\n",
    "                    else:\n",
    "                        v = pred.est\n",
    "\n",
    "                    if v > curr_max or curr == None:\n",
    "                        curr_max = v\n",
    "                        curr = pred\n",
    "            res.append(curr)\n",
    "            res_ids.append(int(curr.iid))\n",
    "            if np.any(np.where(int(curr.iid) == movieIds)[0]):\n",
    "                res_row_indeces.append(movieIdToRowIndex(int(curr.iid)))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(perUserRecommendations):\n",
    "    #implement evaluation metrics here\n",
    "    # some accuracy metric is a baseline (precision@k, nDCG, MAP,...)\n",
    "    # then implement some beyond-accuracy metric (diversity, novelty, coverage, popularity bias,...)    \n",
    "    # some metrics already implemented somewhere:-) \n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ndcg_score.html\n",
    "\n",
    "    m1 = hit_rate(perUserRecommendations)\n",
    "    m2 = novelty(perUserRecommendations)\n",
    "    m3 = ndcg(perUserRecommendations)\n",
    "\n",
    "    return (m1, m2, m3)\n",
    "\n",
    "def recommend_simple(perUserPredictions, top_k):\n",
    "    # select which items should be recommended\n",
    "    # baseline is selection of top-k items with highest estimated ratingpredict\n",
    "    # you can implement some diversity / novelty / coverage enhancements here\n",
    "    return sorted(perUserPredictions, key=lambda x: x.est, reverse=True)[:top_k]\n",
    "\n",
    "def metricStatistics(perUserMetrics):\n",
    "    # aggregate per-user metrics into an overall statistic\n",
    "    # baseline is mean, but you can be more creative\n",
    "    # one other option (needs results of all hyperparam settings) is to compare how many times the algorithm provided better / worse recommendation than other alternatives\n",
    "    mean = np.mean(perUserMetrics, axis=0)\n",
    "    median = np.median(perUserMetrics, axis=0)\n",
    "    var = np.var(perUserMetrics, axis=0)\n",
    "    std = np.std(perUserMetrics, axis=0)\n",
    "\n",
    "    return np.array([mean, median, var, std])\n",
    "\n",
    "def pickBestVariant(results):\n",
    "    # based on the results of the evaluation, select best-performing method\n",
    "    # do the selection based on individual metrics as baseline\n",
    "    # or think about how to make an aggregated selection based on multiple metrics\n",
    "    # ideally, visualize the results to see the tradeoff between metrics\n",
    "\n",
    "    # currently, expected results.shape = (# algs, # metricStatistics, # evaluation metrics) \n",
    "    \n",
    "    def showHeatmap(metric):\n",
    "        plt.title(metric)\n",
    "        sns.heatmap(resultsDf.pivot(\"algorithm\", \"recommendation\", metric), annot=True, fmt=\"f\")\n",
    "        plt.show()\n",
    "    showHeatmap(\"hit_rate_mean\")\n",
    "    showHeatmap(\"novelty_mean\")\n",
    "    showHeatmap(\"ndcg_mean\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "algs = []\n",
    "for n_factors in [10, 100]:\n",
    "    for reg_all in [0, 0.02]:\n",
    "        algs.append((f\"SVD_N={n_factors}_reg={reg_all}\", SVD(n_factors=n_factors, reg_all=reg_all)))\n",
    "\n",
    "for k in [10, 50]:\n",
    "    for sim in ['MSD', 'cosine']:\n",
    "        algs.append((f\"KNN_k={k}_sim={sim}\", KNNBaseline(k=k, sim_options={ 'name': sim })))\n",
    "\n",
    "for _, alg in algs:\n",
    "    alg.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendAlgs = [\n",
    "    (\"Simple\", recommend_simple)\n",
    " ]\n",
    "for alpha in [0.15, 0.5, 0.85]:\n",
    "    #recommendAlgs.append((f\"Diverse-{alpha}\", lambda perUserPredictions, top_k: diverse_recommend(perUserPredictions, top_k, alpha)))\n",
    "    recommendAlgs.append((f\"Diverse_{alpha}\", DiverseRecommend(alpha)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alg = SVD_N=10_reg=0\t rec = Simple\n",
      "[[4.18032787e-03 2.74950170e+00 2.91500712e-02]\n",
      " [0.00000000e+00 2.74375179e+00 0.00000000e+00]\n",
      " [1.91541252e-04 1.10408169e-01 1.21150686e-02]\n",
      " [1.38398429e-02 3.32277247e-01 1.10068472e-01]]\n",
      "******* took: 71.0216715335846\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# todo use some hyperparameter tuning loop here\n",
    "# ideally, try more than one algorithm\n",
    "\n",
    "results = []\n",
    "\n",
    "for algName, alg in algs:\n",
    "    for recName, recommend in recommendAlgs:\n",
    "        start = time.time()\n",
    "        metricsPerUser = []\n",
    "        for uid in allTestSetUsers:\n",
    "            perUserPredictions = []\n",
    "            for oid in allObjects:\n",
    "                if trainset.knows_item(oid):\n",
    "                    print(oid, alg.predict(uid,oid, clip=False))\n",
    "                perUserPredictions.append(alg.predict(uid,oid, clip=False))\n",
    "\n",
    "            recs = recommend(perUserPredictions, 20)\n",
    "            #for rec in recs:\n",
    "            #    print(rec)\n",
    "            #break\n",
    "\n",
    "            (m1, m2, m3) = evaluate(recs)\n",
    "            #print(f\"hit rate: {m1} || novelty: {m2} || ndcg: {m3}\")\n",
    "            # it may be necessary to collect additional information for evaluate() e.g. known ratings similarity matrix etc.\n",
    "            metricsPerUser.append(np.array([m1, m2, m3]))\n",
    "\n",
    "        metricsPerUser = np.array(metricsPerUser)\n",
    "        results_per_alg = metricStatistics(metricsPerUser)\n",
    "        print(f\"alg = {algName}\\t rec = {recName}\")\n",
    "        print(results_per_alg)\n",
    "        print(f\"******* took: {time.time() - start}\\n\\n\")\n",
    "\n",
    "        # accumulate m1, m2 to sth. like metricsPerUser\n",
    "        results.append((algName, recName, results_per_alg))\n",
    "\n",
    "# results = metricStatistics(metricsPerUser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"results.pickle\", \"wb\") as output_file:\n",
    "    pickle.dump(results, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsDf = pd.DataFrame({\"algorithm\": list(map(lambda x: x[0], results)), \n",
    "                          \"recommendation\": list(map(lambda x: x[1], results)), \n",
    "                          \"hit_rate_mean\": list(map(lambda x: x[2][0,0], results)),\n",
    "                          \"novelty_mean\": list(map(lambda x: x[2][0,1], results)),\n",
    "                          \"ndcg_mean\": list(map(lambda x: x[2][0,2], results)) })\n",
    "print(resultsDf)\n",
    "pickBestVariant(resultsDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "user: 1          item: 2          r_ui = None   est = 4.15   {'actual_k': 40, 'was_impossible': False}\n",
      "1 2 4.148609180175668\n"
     ]
    }
   ],
   "source": [
    "# Use the knn.\n",
    "algoKNN = KNNBaseline()\n",
    "algoKNN.fit(trainset)\n",
    "pred = algoKNN.predict(\"1\",\"2\", clip=False)\n",
    "print(pred)\n",
    "print(pred.uid, pred.iid, pred.est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
