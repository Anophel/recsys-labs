{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import KNNBaseline\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise.similarities import cosine\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('ml-latest-small/ratings.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "moviesDF = pd.read_csv(\"ml-latest-small/movies.csv\", sep=\",\")\n",
    "moviesDF.movieId = moviesDF.movieId.astype(int)\n",
    "moviesDF.set_index(\"movieId\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplification of the evaluation case: predict last-k for each user\n",
    "dfTrain = pd.DataFrame()\n",
    "dfTest = pd.DataFrame()\n",
    "for currUser in df.userId.unique():\n",
    "    dataCurrUser = df[df.userId == currUser]\n",
    "    currUserTrain = dataCurrUser.iloc[:-10]\n",
    "    currUserTest = dataCurrUser.iloc[-10:]\n",
    "    dfTrain = dfTrain.append(currUserTrain)\n",
    "    dfTest = dfTest.append(currUserTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '3', '6', ..., '160836', '163937', '163981'], dtype='<U21')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allObjects = df.movieId.unique().astype(str)\n",
    "allTestSetUsers = dfTest.userId.unique().astype(str)\n",
    "allObjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain.to_csv('ml-latest-small/ratingsTrain.csv', index=False)\n",
    "dfTest.to_csv('ml-latest-small/ratingsTest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'ml-latest-small/ratingsTrain.csv'\n",
    "reader = Reader(line_format='user item rating timestamp', sep=',', skip_lines=1)\n",
    "\n",
    "data = Dataset.load_from_file(file_path, reader=reader)\n",
    "trainset = data.build_full_trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def novelty(perUserRecommendations):\n",
    "  # novelty = -log(num_users who have rated the item / num_users)\n",
    "  num_users = len(dfTrain.userId.unique())\n",
    "  acc_novelty = 0\n",
    "  for pred in perUserRecommendations:\n",
    "    num_rated = len(dfTrain[dfTrain[\"movieId\"] == int(pred.iid)])\n",
    "    acc_novelty -= np.log2(num_rated/num_users)\n",
    "  # return average novelty\n",
    "  return acc_novelty / len(perUserRecommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(perUserRecommendations):\n",
    "  hits = 0\n",
    "  for pred in perUserRecommendations:\n",
    "    if ((dfTest['userId'] == int(pred.uid)) & (dfTest['movieId'] == int(pred.iid))).any():\n",
    "      hits += 1\n",
    "  return hits / len(perUserRecommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_relevance_scores(recommendations: list) -> list:\n",
    "  result = []\n",
    "  for recommendation in recommendations:\n",
    "    ratingDf = dfTest[dfTest.userId == int(recommendation.uid)]\n",
    "    ratingDf = ratingDf[ratingDf.movieId == int(recommendation.iid)]\n",
    "\n",
    "    # recommendation iif is relevant if has been reviewed by the user uid\n",
    "    relevance = 1.0 if not ratingDf.empty else 0.0\n",
    "    result.append(relevance)\n",
    "\n",
    "  return np.array(result)\n",
    "\n",
    "def ndcg(perUserRecommendations):\n",
    "\n",
    "  def dcg(rel_scores):\n",
    "    pos = np.arange(1, rel_scores.shape[0] + 1)\n",
    "    return np.sum(\n",
    "      rel_scores / np.log2(pos + 1)\n",
    "    )\n",
    "\n",
    "  relevance_scores = binary_relevance_scores(perUserRecommendations)\n",
    "  hits = int(np.sum(relevance_scores))\n",
    "  # all relevant movies are placed first\n",
    "  ideal_relevance_scores = np.array(\n",
    "    ([1] * hits) + ([0] * (len(perUserRecommendations) - hits))\n",
    "  )\n",
    "  actual_dcg, ideal_dcg = dcg(relevance_scores), dcg(ideal_relevance_scores)\n",
    "\n",
    "  if actual_dcg == 0.0:\n",
    "    return 0.0\n",
    "\n",
    "  return actual_dcg / ideal_dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(perUserRecommendations):\n",
    "    #implement evaluation metrics here\n",
    "    # some accuracy metric is a baseline (precision@k, nDCG, MAP,...)\n",
    "    # then implement some beyond-accuracy metric (diversity, novelty, coverage, popularity bias,...)    \n",
    "    # some metrics already implemented somewhere:-) \n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ndcg_score.html\n",
    "\n",
    "    m1 = hit_rate(perUserRecommendations)\n",
    "    m2 = novelty(perUserRecommendations)\n",
    "    m3 = ndcg(perUserRecommendations)\n",
    "\n",
    "    return (m1, m2, m3)\n",
    "\n",
    "def recommend(perUserPredictions, top_k):\n",
    "    # select which items should be recommended\n",
    "    # baseline is selection of top-k items with highest estimated ratingpredict\n",
    "    # you can implement some diversity / novelty / coverage enhancements here\n",
    "    return sorted(perUserPredictions, key=lambda x: x.est, reverse=True)[:top_k]\n",
    "\n",
    "def metricStatistics(perUserMetrics):\n",
    "    # aggregate per-user metrics into an overall statistic\n",
    "    # baseline is mean, but you can be more creative\n",
    "    # one other option (needs results of all hyperparam settings) is to compare how many times the algorithm provided better / worse recommendation than other alternatives\n",
    "    mean = np.mean(perUserMetrics, axis=0)\n",
    "    median = np.median(perUserMetrics, axis=0)\n",
    "    var = np.var(perUserMetrics, axis=0)\n",
    "    std = np.std(perUserMetrics, axis=0)\n",
    "\n",
    "    return np.array([mean, median, var, std])\n",
    "\n",
    "def pickBestVariant(results):\n",
    "    # based on the results of the evaluation, select best-performing method\n",
    "    # do the selection based on individual metrics as baseline\n",
    "    # or think about how to make an aggregated selection based on multiple metrics\n",
    "    # ideally, visualize the results to see the tradeoff between metrics\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "algs = [SVD(), KNNBaseline()]\n",
    "for alg in algs:\n",
    "    alg.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: 1          item: 246        r_ui = None   est = 5.22   {'was_impossible': False}\n",
      "user: 1          item: 541        r_ui = None   est = 5.22   {'was_impossible': False}\n",
      "user: 1          item: 1276       r_ui = None   est = 5.19   {'was_impossible': False}\n",
      "user: 1          item: 2160       r_ui = None   est = 5.18   {'was_impossible': False}\n",
      "user: 1          item: 7361       r_ui = None   est = 5.13   {'was_impossible': False}\n",
      "user: 1          item: 5690       r_ui = None   est = 5.13   {'was_impossible': False}\n",
      "user: 1          item: 318        r_ui = None   est = 5.11   {'was_impossible': False}\n",
      "user: 1          item: 750        r_ui = None   est = 5.10   {'was_impossible': False}\n",
      "user: 1          item: 3468       r_ui = None   est = 5.08   {'was_impossible': False}\n",
      "user: 1          item: 1225       r_ui = None   est = 5.07   {'was_impossible': False}\n",
      "user: 1          item: 2019       r_ui = None   est = 5.07   {'was_impossible': False}\n",
      "user: 1          item: 4993       r_ui = None   est = 5.07   {'was_impossible': False}\n",
      "user: 1          item: 6016       r_ui = None   est = 5.06   {'was_impossible': False}\n",
      "user: 1          item: 7153       r_ui = None   est = 5.06   {'was_impossible': False}\n",
      "user: 1          item: 38061      r_ui = None   est = 5.06   {'was_impossible': False}\n",
      "user: 1          item: 260        r_ui = None   est = 5.06   {'was_impossible': False}\n",
      "user: 1          item: 56782      r_ui = None   est = 5.05   {'was_impossible': False}\n",
      "user: 1          item: 720        r_ui = None   est = 5.05   {'was_impossible': False}\n",
      "user: 1          item: 3147       r_ui = None   est = 5.05   {'was_impossible': False}\n",
      "user: 1          item: 916        r_ui = None   est = 5.05   {'was_impossible': False}\n",
      "user: 1          item: 148881     r_ui = None   est = 6.95   {'actual_k': 1, 'was_impossible': False}\n",
      "user: 1          item: 40491      r_ui = None   est = 6.95   {'actual_k': 1, 'was_impossible': False}\n",
      "user: 1          item: 156605     r_ui = None   est = 6.45   {'actual_k': 1, 'was_impossible': False}\n",
      "user: 1          item: 5490       r_ui = None   est = 6.29   {'actual_k': 1, 'was_impossible': False}\n",
      "user: 1          item: 132333     r_ui = None   est = 6.29   {'actual_k': 1, 'was_impossible': False}\n",
      "user: 1          item: 25947      r_ui = None   est = 6.21   {'actual_k': 1, 'was_impossible': False}\n",
      "user: 1          item: 3379       r_ui = None   est = 6.04   {'actual_k': 1, 'was_impossible': False}\n",
      "user: 1          item: 6611       r_ui = None   est = 5.99   {'actual_k': 1, 'was_impossible': False}\n",
      "user: 1          item: 100556     r_ui = None   est = 5.99   {'actual_k': 1, 'was_impossible': False}\n",
      "user: 1          item: 152711     r_ui = None   est = 5.99   {'actual_k': 1, 'was_impossible': False}\n",
      "user: 1          item: 160644     r_ui = None   est = 5.99   {'actual_k': 1, 'was_impossible': False}\n",
      "user: 1          item: 162414     r_ui = None   est = 5.99   {'actual_k': 1, 'was_impossible': False}\n",
      "user: 1          item: 167064     r_ui = None   est = 5.99   {'actual_k': 1, 'was_impossible': False}\n",
      "user: 1          item: 67618      r_ui = None   est = 5.99   {'actual_k': 1, 'was_impossible': False}\n",
      "user: 1          item: 8477       r_ui = None   est = 5.98   {'actual_k': 2, 'was_impossible': False}\n",
      "user: 1          item: 3567       r_ui = None   est = 5.97   {'actual_k': 1, 'was_impossible': False}\n",
      "user: 1          item: 57502      r_ui = None   est = 5.95   {'actual_k': 1, 'was_impossible': False}\n",
      "user: 1          item: 3086       r_ui = None   est = 5.91   {'actual_k': 1, 'was_impossible': False}\n",
      "user: 1          item: 4495       r_ui = None   est = 5.86   {'actual_k': 1, 'was_impossible': False}\n",
      "user: 1          item: 6201       r_ui = None   est = 5.86   {'actual_k': 1, 'was_impossible': False}\n"
     ]
    }
   ],
   "source": [
    "# todo use some hyperparameter tuning loop here\n",
    "# ideally, try more than one algorithm\n",
    "\n",
    "results = []\n",
    "\n",
    "for alg in algs:\n",
    "    metricsPerUser = []\n",
    "    for uid in allTestSetUsers:\n",
    "        perUserPredictions = []\n",
    "        for oid in allObjects:\n",
    "            perUserPredictions.append(alg.predict(uid,oid, clip=False))\n",
    "\n",
    "        recs = recommend(perUserPredictions, 20)\n",
    "        #for rec in recs:\n",
    "        #    print(rec)\n",
    "        #break\n",
    "\n",
    "        (m1, m2, m3) = evaluate(recs)\n",
    "        print(f\"{m1} || {m2} || {m3}\")\n",
    "        # it may be necessary to collect additional information for evaluate() e.g. known ratings similarity matrix etc.\n",
    "        metricsPerUser.append(np.array([m1, m2, m3]))\n",
    "\n",
    "    metricsPerUser = np.array(metricsPerUser)\n",
    "    results_per_alg = metricStatistics(metricsPerUser)\n",
    "    \n",
    "    # accumulate m1, m2 to sth. like metricsPerUser\n",
    "    results.append(results_per_alg)\n",
    "\n",
    "pickBestVariant(np.array(results))\n",
    "# results = metricStatistics(metricsPerUser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "user: 1          item: 2          r_ui = None   est = 4.15   {'actual_k': 40, 'was_impossible': False}\n",
      "1 2 4.148609180175668\n"
     ]
    }
   ],
   "source": [
    "# Use the knn.\n",
    "algoKNN = KNNBaseline()\n",
    "algoKNN.fit(trainset)\n",
    "pred = algoKNN.predict(\"1\",\"2\", clip=False)\n",
    "print(pred)\n",
    "print(pred.uid, pred.iid, pred.est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
