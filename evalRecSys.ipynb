{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import KNNBaseline\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise.similarities import cosine\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('ml-latest-small/ratings.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "moviesDF = pd.read_csv(\"ml-latest-small/movies.csv\", sep=\",\")\n",
    "moviesDF.movieId = moviesDF.movieId.astype(int)\n",
    "moviesDF.set_index(\"movieId\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplification of the evaluation case: predict last-k for each user\n",
    "dfTrain = pd.DataFrame()\n",
    "dfTest = pd.DataFrame()\n",
    "for currUser in df.userId.unique():\n",
    "    dataCurrUser = df[df.userId == currUser]\n",
    "    currUserTrain = dataCurrUser.iloc[:-10]\n",
    "    currUserTest = dataCurrUser.iloc[-10:]\n",
    "    dfTrain = dfTrain.append(currUserTrain)\n",
    "    dfTest = dfTest.append(currUserTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '3', '6', ..., '160836', '163937', '163981'], dtype='<U21')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allObjects = df.movieId.unique().astype(str)\n",
    "allTestSetUsers = dfTest.userId.unique().astype(str)\n",
    "allObjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain.to_csv('ml-latest-small/ratingsTrain.csv', index=False)\n",
    "dfTest.to_csv('ml-latest-small/ratingsTest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'ml-latest-small/ratingsTrain.csv'\n",
    "reader = Reader(line_format='user item rating timestamp', sep=',', skip_lines=1)\n",
    "\n",
    "data = Dataset.load_from_file(file_path, reader=reader)\n",
    "trainset = data.build_full_trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def novelty(perUserRecommendations):\n",
    "    # novelty = -log(num_users who have rated the item / num_users)\n",
    "    num_users = len(dfTrain.userId.unique())\n",
    "    acc_novelty = 0\n",
    "    for pred in perUserRecommendations:\n",
    "        num_rated = len(df[df[\"movieId\"] == int(pred.iid)])\n",
    "        acc_novelty -= np.log2(num_rated/num_users)\n",
    "    # return average novelty\n",
    "    return acc_novelty / len(perUserRecommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(perUserRecommendations):\n",
    "    hits = 0\n",
    "    for pred in perUserRecommendations:\n",
    "        if ((dfTest['userId'] == int(pred.uid)) & (dfTest['movieId'] == int(pred.iid))).any():\n",
    "            hits += 1\n",
    "    return hits / len(perUserRecommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_relevance_scores(recommendations: list) -> list:\n",
    "    result = []\n",
    "    for recommendation in recommendations:\n",
    "        ratingDf = dfTest[dfTest.userId == int(recommendation.uid)]\n",
    "        ratingDf = ratingDf[ratingDf.movieId == int(recommendation.iid)]\n",
    "\n",
    "        # recommendation iif is relevant if has been reviewed by the user uid\n",
    "        relevance = 1.0 if not ratingDf.empty else 0.0\n",
    "        result.append(relevance)\n",
    "\n",
    "    return np.array(result)\n",
    "\n",
    "def ndcg(perUserRecommendations):\n",
    "\n",
    "    def dcg(rel_scores):\n",
    "        pos = np.arange(1, rel_scores.shape[0] + 1)\n",
    "        return np.sum(\n",
    "          rel_scores / np.log2(pos + 1)\n",
    "        )\n",
    "\n",
    "    relevance_scores = binary_relevance_scores(perUserRecommendations)\n",
    "    hits = int(np.sum(relevance_scores))\n",
    "    # all relevant movies are placed first\n",
    "    ideal_relevance_scores = np.array(\n",
    "        ([1] * hits) + ([0] * (len(perUserRecommendations) - hits))\n",
    "    )\n",
    "    actual_dcg, ideal_dcg = dcg(relevance_scores), dcg(ideal_relevance_scores)\n",
    "\n",
    "    if actual_dcg == 0.0:\n",
    "        return 0.0\n",
    "\n",
    "    return actual_dcg / ideal_dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Precompute dense rating matrix\n",
    "movieIds = dfTrain['movieId'].unique()\n",
    "userIds = dfTrain['userId'].unique()\n",
    "\n",
    "def movieIdToRowIndex(i):\n",
    "    return np.where(i == movieIds)[0][0]\n",
    "def userIdToRowIndex(i):\n",
    "    return np.where(i == userIds)[0][0]\n",
    "\n",
    "movieIdsMapped = np.array(list(map(movieIdToRowIndex, dfTrain['movieId'].tolist())), dtype=np.int64).flatten()\n",
    "userIdsMapped = np.array(list(map(userIdToRowIndex, dfTrain['userId'].tolist())), dtype=np.int64).flatten()\n",
    "rating_mat = np.zeros((movieIds.shape[0], userIds.shape[0]), dtype=np.int8)\n",
    "\n",
    "rating_mat[movieIdsMapped, userIdsMapped] = dfTrain[\"rating\"].tolist()\n",
    "rating_mat = rating_mat / (np.sum(rating_mat ** 2, axis=1) ** 0.5).reshape((-1,1))\n",
    "\n",
    "def cosine_distance(a, b):\n",
    "    return (1 - np.dot(a,b)) / 2 # expecting normalized vectors\n",
    "            \n",
    "def diverse_recommend(perUserPredictions, top_k, alpha=0.5):\n",
    "    max_est = max(map(lambda x: x.est, perUserPredictions))\n",
    "    res = []\n",
    "    res_ids = []\n",
    "    res_row_indeces = []\n",
    "    for i in range(top_k):\n",
    "        curr_max = 0\n",
    "        curr = None\n",
    "        for pred in perUserPredictions:\n",
    "            predIid = int(pred.iid)\n",
    "            if predIid not in res_ids:\n",
    "                max_cos = 0\n",
    "                # Compute diversity only for the extended result and we have ranking vector\n",
    "                # for the prediction\n",
    "                if len(res_row_indeces) > 0 and np.any(np.where(predIid == movieIds)[0]):\n",
    "                    max_cos = np.max(cosine_distance(rating_mat[movieIdToRowIndex(predIid)], rating_mat[res_row_indeces].T)) / 2\n",
    "                    v = alpha * (pred.est / max_est) - (1 - alpha) * max_cos\n",
    "                else:\n",
    "                    v = pred.est / max_est\n",
    "                \n",
    "                if v > curr_max or curr == None:\n",
    "                    curr_max = v\n",
    "                    curr = pred\n",
    "        res.append(curr)\n",
    "        res_ids.append(int(curr.iid))\n",
    "        if np.any(np.where(int(curr.iid) == movieIds)[0]):\n",
    "            res_row_indeces.append(movieIdToRowIndex(int(curr.iid)))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(perUserRecommendations):\n",
    "    #implement evaluation metrics here\n",
    "    # some accuracy metric is a baseline (precision@k, nDCG, MAP,...)\n",
    "    # then implement some beyond-accuracy metric (diversity, novelty, coverage, popularity bias,...)    \n",
    "    # some metrics already implemented somewhere:-) \n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ndcg_score.html\n",
    "\n",
    "    m1 = hit_rate(perUserRecommendations)\n",
    "    m2 = novelty(perUserRecommendations)\n",
    "    m3 = ndcg(perUserRecommendations)\n",
    "\n",
    "    return (m1, m2, m3)\n",
    "\n",
    "def recommend_simple(perUserPredictions, top_k):\n",
    "    # select which items should be recommended\n",
    "    # baseline is selection of top-k items with highest estimated ratingpredict\n",
    "    # you can implement some diversity / novelty / coverage enhancements here\n",
    "    return sorted(perUserPredictions, key=lambda x: x.est, reverse=True)[:top_k]\n",
    "\n",
    "def metricStatistics(perUserMetrics):\n",
    "    # aggregate per-user metrics into an overall statistic\n",
    "    # baseline is mean, but you can be more creative\n",
    "    # one other option (needs results of all hyperparam settings) is to compare how many times the algorithm provided better / worse recommendation than other alternatives\n",
    "    mean = np.mean(perUserMetrics, axis=0)\n",
    "    median = np.median(perUserMetrics, axis=0)\n",
    "    var = np.var(perUserMetrics, axis=0)\n",
    "    std = np.std(perUserMetrics, axis=0)\n",
    "\n",
    "    return np.array([mean, median, var, std])\n",
    "\n",
    "def pickBestVariant(results):\n",
    "    # based on the results of the evaluation, select best-performing method\n",
    "    # do the selection based on individual metrics as baseline\n",
    "    # or think about how to make an aggregated selection based on multiple metrics\n",
    "    # ideally, visualize the results to see the tradeoff between metrics\n",
    "\n",
    "    # currently, expected results.shape = (# algs, # metricStatistics, # evaluation metrics) \n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algs = [(\"SVD-100\", SVD(n_factors=100)), (\"SVD-10\", SVD(n_factors=10)), (\"KNN\", KNNBaseline())]\n",
    "for _, alg in algs:\n",
    "    alg.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendAlgs = [\n",
    "    (\"Diverse-0.5\", lambda perUserPredictions, top_k: diverse_recommend(perUserPredictions, top_k, 0.5)), \n",
    "    (\"Diverse-0.8\", lambda perUserPredictions, top_k: diverse_recommend(perUserPredictions, top_k, 0.8)), \n",
    "    (\"Simple\", recommend_simple)\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo use some hyperparameter tuning loop here\n",
    "# ideally, try more than one algorithm\n",
    "\n",
    "results = []\n",
    "\n",
    "for algName, alg in algs:\n",
    "    for recName, recommend in recommendAlgs:\n",
    "        metricsPerUser = []\n",
    "        for uid in allTestSetUsers[:10]:\n",
    "            perUserPredictions = []\n",
    "            for oid in allObjects:\n",
    "                perUserPredictions.append(alg.predict(uid,oid, clip=False))\n",
    "\n",
    "            recs = recommend(perUserPredictions, 20)\n",
    "            #for rec in recs:\n",
    "            #    print(rec)\n",
    "            #break\n",
    "\n",
    "            (m1, m2, m3) = evaluate(recs)\n",
    "            print(f\"hit rate: {m1} || novelty: {m2} || ndcg: {m3}\")\n",
    "            # it may be necessary to collect additional information for evaluate() e.g. known ratings similarity matrix etc.\n",
    "            metricsPerUser.append(np.array([m1, m2, m3]))\n",
    "\n",
    "        metricsPerUser = np.array(metricsPerUser)\n",
    "        results_per_alg = metricStatistics(metricsPerUser)\n",
    "        print(results_per_alg)\n",
    "\n",
    "        # accumulate m1, m2 to sth. like metricsPerUser\n",
    "        results.append((algName, recName, results_per_alg))\n",
    "\n",
    "pickBestVariant(np.array(results))\n",
    "# results = metricStatistics(metricsPerUser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "user: 1          item: 2          r_ui = None   est = 4.15   {'actual_k': 40, 'was_impossible': False}\n",
      "1 2 4.148609180175668\n"
     ]
    }
   ],
   "source": [
    "# Use the knn.\n",
    "algoKNN = KNNBaseline()\n",
    "algoKNN.fit(trainset)\n",
    "pred = algoKNN.predict(\"1\",\"2\", clip=False)\n",
    "print(pred)\n",
    "print(pred.uid, pred.iid, pred.est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
